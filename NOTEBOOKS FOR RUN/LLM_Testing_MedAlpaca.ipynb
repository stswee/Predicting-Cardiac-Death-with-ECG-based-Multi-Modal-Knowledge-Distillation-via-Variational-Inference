{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7aecd3-b366-42aa-a2c7-0eaa0d9f48dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sswee/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 2\n",
      "GPU 0: NVIDIA A100-SXM4-40GB\n",
      "GPU 1: NVIDIA A100-SXM4-40GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9f1bf5a650>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import json\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import transformers\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Set GPUs\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "\n",
    "print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Set seeds\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1519726d-c1e0-49cf-8368-e5d36349dbf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/tiktoken/load.py:11\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(blobpath)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mblobfile\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'blobfile'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1725\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1721\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1622\u001b[0m, in \u001b[0;36mTikTokenConverter.converted\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[0;32m-> 1622\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m pre_tokenizers\u001b[38;5;241m.\u001b[39mSequence(\n\u001b[1;32m   1624\u001b[0m         [\n\u001b[1;32m   1625\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mSplit(Regex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern), behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misolated\u001b[39m\u001b[38;5;124m\"\u001b[39m, invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1626\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space, use_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1627\u001b[0m         ]\n\u001b[1;32m   1628\u001b[0m     )\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1615\u001b[0m, in \u001b[0;36mTikTokenConverter.tokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1615\u001b[0m     vocab_scores, merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1616\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(BPE(vocab_scores, merges, fuse_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1591\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[0;34m(self, tiktoken_url)\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tiktoken` is required to read a `tiktoken` file. Install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tiktoken`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[0;32m-> 1591\u001b[0m bpe_ranks \u001b[38;5;241m=\u001b[39m \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1592\u001b[0m byte_encoder \u001b[38;5;241m=\u001b[39m bytes_to_unicode()\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/tiktoken/load.py:148\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file, expected_hash)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m, expected_hash: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     contents \u001b[38;5;241m=\u001b[39m \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     ret \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/tiktoken/load.py:63\u001b[0m, in \u001b[0;36mread_file_cached\u001b[0;34m(blobpath, expected_hash)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblobpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_hash \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_hash(contents, expected_hash):\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/tiktoken/load.py:13\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(blobpath)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblobfile is not installed. Please install it by running `pip install blobfile`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m blobfile\u001b[38;5;241m.\u001b[39mBlobFile(blobpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mImportError\u001b[0m: blobfile is not installed. Please install it by running `pip install blobfile`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# # Load Huggingface Model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(model_name)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m---> 15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedalpaca/medalpaca-7b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedalpaca/medalpaca-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:944\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    942\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m         )\n\u001b[0;32m--> 944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2052\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2061\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2292\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2292\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2294\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2295\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2297\u001b[0m     )\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py:157\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, legacy, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_prefix_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_slow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_bos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_eos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:139\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m--> 139\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1727\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[1;32m   1723\u001b[0m         vocab_file\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39mvocab_file,\n\u001b[1;32m   1724\u001b[0m         additional_special_tokens\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39madditional_special_tokens,\n\u001b[1;32m   1725\u001b[0m     )\u001b[38;5;241m.\u001b[39mconverted()\n\u001b[1;32m   1726\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1729\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1730\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently available slow->fast convertors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1731\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "# Log into Huggingface\n",
    "with open(\"../../huggingface_token.txt\", \"r\") as file:\n",
    "    access_token = file.read().strip()\n",
    "login(access_token)\n",
    "\n",
    "# # Load Huggingface Model\n",
    "# model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, token=access_token, low_cpu_mem_usage=True,\n",
    "#                     torch_dtype=torch.float16, device_map='auto')\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"medalpaca/medalpaca-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"medalpaca/medalpaca-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52d84325-93c7-4fd1-a003-cf89f8405b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message(note):\n",
    "    system = 'You are a medical assistant. Your tasks are to generate a clinical note and create an Assessment and Plan section. Additionally, you will also determine the fate of the patient. The patient either survives for the next few years or succumbs to either sudden cardiac death or pump failure death. Only suggest death if there is strong evidence for it. Provide your confidence in survival, sudden cardiac death, and pump failure death such that the confidence percentages add up to 100 percent and format these results in a list. Please output a clinical note that has a section for demographics, medical history, lab results, LVEF, medication, and ECG impressions. In the end, put the Assessment and Plan section along with a prediction. Provide reasoning for the prediction.'\n",
    "\n",
    "    prompt = f\"Here is the patient data: \\n{note}\"\n",
    "\n",
    "    messages = [\n",
    "\t\t{\"role\": \"system\", \"content\": system},\n",
    "\t\t{\"role\": \"user\", \"content\": prompt}\n",
    "\t]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def extract_assistant_response(response):\n",
    "    parts = response.split(\"assistant\\n\\n\", 1)\n",
    "    return parts[1].strip() if len(parts) > 1 else response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "609ef6f5-fbfe-415b-98d3-115bfee5b9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generate a structured clinical note based on the following data:\\n\\nAge: 58\\nGender: Male \\nWeight: 83 kg\\nHeight: 163 cm\\nNYHA Class: III\\nBlood Pressure: 110/75 mmHg\\nPast Medical History: Idiopathic dilated cardiomyopathy\\nAlbumin (g/L): 424.0\\nALT or GPT (IU/L): 10\\nAST or GOT (IU/L): 20\\nTotal Cholesterol (mmol/L): 54\\nCreatinine (mmol/L): 106\\nGamma-glutamil transpeptidase (IU/L): 20.0\\nGlucose (mmol/L): 57.0\\nHemoglobin (g/L): 132.0\\nHDL (mmol/L): 1,29\\nPotassium (mEq/L): 46.0\\nLDL (mmol/L): 3,36\\nSodium (mEq/L): 141.0\\nPro-BNP (ng/L): 1834.0\\nProtein (g/L): 69.0\\nT3 (pg/dL): 0,05\\nT4 (ng/L): 15.0\\nTroponin (ng/mL): 0,01\\nTSH (mIU/L): 3,02\\nUrea (mg/dL): 712\\nLVEF (%): 35\\nMedications: Beta Blockers, Digoxin, Loop Diuretics, ACE Inhibitor\\nHolter rhythm: permanent atrial fibrillation \\nECG Impression:\\n        - Ventricular Extrasystole: Polymorphic\\n        - Ventricular Tachycardia: Non-sustained VT\\n        - Non-sustained ventricular tachycardia (CH>10): Yes\\n        - Paroxysmal supraventricular tachyarrhythmia: Unknown paroxysmal supraventricular tachyarrhythmia code\\n        - Bradycardia: Unknown bradycardia code\\n            '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in csv file with prompts\n",
    "df = pd.read_csv(\"../Data/subject-info-cleaned-with-prompts.csv\")\n",
    "df['Prompts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f853ae3-e46b-4b53-bef1-c6d391d3ddd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a medical assistant. Your tasks are to generate a clinical note and create an Assessment and Plan section. Additionally, you will also determine the fate of the patient. The patient either survives for the next few years or succumbs to either sudden cardiac death or pump failure death. Only suggest death if there is strong evidence for it. Provide your confidence in survival, sudden cardiac death, and pump failure death such that the confidence percentages add up to 100 percent and format these results in a list. Please output a clinical note that has a section for demographics, medical history, lab results, LVEF, medication, and ECG impressions. In the end, put the Assessment and Plan section along with a prediction. Provide reasoning for the prediction.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Here is the patient data: \\nGenerate a structured clinical note based on the following data:\\n\\nAge: 58\\nGender: Male \\nWeight: 83 kg\\nHeight: 163 cm\\nNYHA Class: III\\nBlood Pressure: 110/75 mmHg\\nPast Medical History: Idiopathic dilated cardiomyopathy\\nAlbumin (g/L): 424.0\\nALT or GPT (IU/L): 10\\nAST or GOT (IU/L): 20\\nTotal Cholesterol (mmol/L): 54\\nCreatinine (mmol/L): 106\\nGamma-glutamil transpeptidase (IU/L): 20.0\\nGlucose (mmol/L): 57.0\\nHemoglobin (g/L): 132.0\\nHDL (mmol/L): 1,29\\nPotassium (mEq/L): 46.0\\nLDL (mmol/L): 3,36\\nSodium (mEq/L): 141.0\\nPro-BNP (ng/L): 1834.0\\nProtein (g/L): 69.0\\nT3 (pg/dL): 0,05\\nT4 (ng/L): 15.0\\nTroponin (ng/mL): 0,01\\nTSH (mIU/L): 3,02\\nUrea (mg/dL): 712\\nLVEF (%): 35\\nMedications: Beta Blockers, Digoxin, Loop Diuretics, ACE Inhibitor\\nHolter rhythm: permanent atrial fibrillation \\nECG Impression:\\n        - Ventricular Extrasystole: Polymorphic\\n        - Ventricular Tachycardia: Non-sustained VT\\n        - Non-sustained ventricular tachycardia (CH>10): Yes\\n        - Paroxysmal supraventricular tachyarrhythmia: Unknown paroxysmal supraventricular tachyarrhythmia code\\n        - Bradycardia: Unknown bradycardia code\\n            '}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = get_message(df['Prompts'][0])\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "859859f7-9443-449c-adca-9777693cbe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Put message into LLM\n",
    "input_text = tokenizer.apply_chat_template(message, tokenize = False, add_generation_prompt = True)\n",
    "inputs = tokenizer(input_text, return_tensors = \"pt\").to(model.device)\n",
    "output = model.generate(**inputs, max_new_tokens = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e5bb423-a2e3-4a51-9d6c-0c156152bef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Clinical Note\\n\\nDemographics:\\n\\n- Patient Name: [Insert Patient Name]\\n- Age: 58 years\\n- Gender: Male\\n- Weight: 83 kg\\n- Height: 163 cm\\n\\nMedical History:\\n\\n- Past Medical History: Idiopathic dilated cardiomyopathy\\n- Current Medications:\\n  - Beta Blockers\\n  - Digoxin\\n  - Loop Diuretics\\n  - ACE Inhibitor\\n\\nLab Results:\\n\\n- Albumin: 424.0 g/L\\n- ALT or GPT: 10 IU/L\\n- AST or GOT: 20 IU/L\\n- Total Cholesterol: 54 mmol/L\\n- Creatinine: 106 mmol/L\\n- Gamma-glutamil transpeptidase: 20.0 IU/L\\n- Glucose: 57.0 mmol/L\\n- Hemoglobin: 132.0 g/L\\n- HDL: 1.29 mmol/L\\n- Potassium: 4.6 mEq/L\\n- LDL: 3.36 mmol/L\\n- Sodium: 141.0 mEq/L\\n- Pro-BNP: 1834.0 ng/L\\n- Protein: 69.0 g/L\\n- T3: 0.05 pg/dL\\n- T4: 15.0 ng/L\\n- Troponin: 0.01 ng/mL\\n- TSH: 3.02 mIU/L\\n- Urea: 7.12 mg/dL\\n\\nLVEF:\\n\\n- Left Ventricular Ejection Fraction (LVEF): 35%\\n\\nECG Impression:\\n\\n- Ventricular Extrasystole: Polymorphic\\n- Ventricular Tachycardia: Non-sustained VT\\n- Non-sustained ventricular tachycardia (CH>10): Yes\\n- Paroxysmal supraventricular tachyarrhythmia: Unknown paroxysmal supraventricular tachyarrhythmia code\\n- Bradycardia: Unknown bradycardia code\\n\\nAssessment and Plan:\\n\\nThis patient has a history of idiopathic dilated cardiomyopathy with a significantly reduced LVEF of 35%. He is on beta blockers, digoxin, loop diuretics, and an ACE inhibitor. His lab results show elevated pro-BNP levels, indicating heart failure. The presence of non-sustained ventricular tachycardia and polymorphic ventricular extrasystoles on ECG suggests a high risk of arrhythmias.\\n\\nBased on the patient's history, lab results, and ECG findings, I assess his condition as follows:\\n\\n- Survival for the next few years: 60%\\n- Sudden cardiac death: 25%\\n- Pump failure death: 15%\\n\\nThe patient's reduced LVEF and elevated pro-BNP levels indicate a high risk of heart failure progression. However, his current medications and the absence of severe electrolyte imbalances or acute kidney injury suggest that he is being managed appropriately. The presence of non-sustained ventricular tachycardia and polymorphic ventricular extrasystoles on ECG increases his risk of sudden cardiac death. Given these factors, I predict that the patient has a moderate risk of survival for the next few years, a higher risk of sudden cardiac death, and a lower risk of pump failure death.\\n\\nReasoning:\\n\\nThe patient's reduced LVEF and elevated pro-BNP levels suggest a high risk of heart failure progression, which increases his risk of pump failure death. However, his current medications and the absence of severe electrolyte imbalances or acute kidney injury suggest that he is being managed appropriately. The presence of non-sustained ventricular tachycardia and polymorphic ventricular extrasystoles on ECG increases his risk of sudden cardiac death. Therefore, I predict that the patient has a moderate risk of survival for the next few years, a higher risk of sudden cardiac death, and a lower risk of pump failure death.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get result\n",
    "result = tokenizer.decode(output[0], skip_special_tokens = True)\n",
    "result = result.replace(\"**\", \"\")\n",
    "result = extract_assistant_response(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2758b1f6-d0b4-428e-b7e6-e2f8fc4332c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical Note\n",
      "\n",
      "Demographics:\n",
      "\n",
      "- Patient Name: [Insert Patient Name]\n",
      "- Age: 58 years\n",
      "- Gender: Male\n",
      "- Weight: 83 kg\n",
      "- Height: 163 cm\n",
      "\n",
      "Medical History:\n",
      "\n",
      "- Past Medical History: Idiopathic dilated cardiomyopathy\n",
      "- Current Medications:\n",
      "  - Beta Blockers\n",
      "  - Digoxin\n",
      "  - Loop Diuretics\n",
      "  - ACE Inhibitor\n",
      "\n",
      "Lab Results:\n",
      "\n",
      "- Albumin: 424.0 g/L\n",
      "- ALT or GPT: 10 IU/L\n",
      "- AST or GOT: 20 IU/L\n",
      "- Total Cholesterol: 54 mmol/L\n",
      "- Creatinine: 106 mmol/L\n",
      "- Gamma-glutamil transpeptidase: 20.0 IU/L\n",
      "- Glucose: 57.0 mmol/L\n",
      "- Hemoglobin: 132.0 g/L\n",
      "- HDL: 1.29 mmol/L\n",
      "- Potassium: 4.6 mEq/L\n",
      "- LDL: 3.36 mmol/L\n",
      "- Sodium: 141.0 mEq/L\n",
      "- Pro-BNP: 1834.0 ng/L\n",
      "- Protein: 69.0 g/L\n",
      "- T3: 0.05 pg/dL\n",
      "- T4: 15.0 ng/L\n",
      "- Troponin: 0.01 ng/mL\n",
      "- TSH: 3.02 mIU/L\n",
      "- Urea: 7.12 mg/dL\n",
      "\n",
      "LVEF:\n",
      "\n",
      "- Left Ventricular Ejection Fraction (LVEF): 35%\n",
      "\n",
      "ECG Impression:\n",
      "\n",
      "- Ventricular Extrasystole: Polymorphic\n",
      "- Ventricular Tachycardia: Non-sustained VT\n",
      "- Non-sustained ventricular tachycardia (CH>10): Yes\n",
      "- Paroxysmal supraventricular tachyarrhythmia: Unknown paroxysmal supraventricular tachyarrhythmia code\n",
      "- Bradycardia: Unknown bradycardia code\n",
      "\n",
      "Assessment and Plan:\n",
      "\n",
      "This patient has a history of idiopathic dilated cardiomyopathy with a significantly reduced LVEF of 35%. He is on beta blockers, digoxin, loop diuretics, and an ACE inhibitor. His lab results show elevated pro-BNP levels, indicating heart failure. The presence of non-sustained ventricular tachycardia and polymorphic ventricular extrasystoles on ECG suggests a high risk of arrhythmias.\n",
      "\n",
      "Based on the patient's history, lab results, and ECG findings, I assess his condition as follows:\n",
      "\n",
      "- Survival for the next few years: 60%\n",
      "- Sudden cardiac death: 25%\n",
      "- Pump failure death: 15%\n",
      "\n",
      "The patient's reduced LVEF and elevated pro-BNP levels indicate a high risk of heart failure progression. However, his current medications and the absence of severe electrolyte imbalances or acute kidney injury suggest that he is being managed appropriately. The presence of non-sustained ventricular tachycardia and polymorphic ventricular extrasystoles on ECG increases his risk of sudden cardiac death. Given these factors, I predict that the patient has a moderate risk of survival for the next few years, a higher risk of sudden cardiac death, and a lower risk of pump failure death.\n",
      "\n",
      "Reasoning:\n",
      "\n",
      "The patient's reduced LVEF and elevated pro-BNP levels suggest a high risk of heart failure progression, which increases his risk of pump failure death. However, his current medications and the absence of severe electrolyte imbalances or acute kidney injury suggest that he is being managed appropriately. The presence of non-sustained ventricular tachycardia and polymorphic ventricular extrasystoles on ECG increases his risk of sudden cardiac death. Therefore, I predict that the patient has a moderate risk of survival for the next few years, a higher risk of sudden cardiac death, and a lower risk of pump failure death.\n"
     ]
    }
   ],
   "source": [
    "test_result = result.replace(\"**\", \"\")\n",
    "print(extract_assistant_response(test_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e488b15a-9f6b-4efe-9467-cd6d7f7c648e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1524402/455679211.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['Reports'] = None\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "df_test = df[[df.columns[1]]]\n",
    "df_test['Reports'] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0b4c99b-0f2a-4670-b7d7-58a0566b6131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.                                                                                  | 0/5 [00:00<?, ?it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.                                                                          | 1/5 [00:39<02:37, 39.41s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.                                                                          | 2/5 [01:11<01:44, 34.89s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.████████████████████                                                      | 3/5 [01:40<01:05, 32.53s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.███████████████████████████████████████████████                           | 4/5 [02:15<00:33, 33.32s/it]\n",
      "Test: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [02:47<00:00, 33.51s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5), desc = \"Test\"):\n",
    "    # Get message\n",
    "    message = get_message(df['Prompts'][i])\n",
    "\n",
    "    # Put message into LLM\n",
    "    input_text = tokenizer.apply_chat_template(message, tokenize = False, add_generation_prompt = True)\n",
    "    inputs = tokenizer(input_text, return_tensors = \"pt\").to(model.device)\n",
    "    output = model.generate(**inputs, max_new_tokens = 1000)\n",
    "\n",
    "    # Get result\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens = True)\n",
    "    result = result.replace(\"**\", \"\")\n",
    "    result = extract_assistant_response(result)\n",
    "\n",
    "    # Store result\n",
    "    df_test.loc[i, 'Reports'] = result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "969e0f1e-6f5a-4c6a-90ed-4b4a3d50ad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical Note\n",
      "\n",
      "Demographics\n",
      "\n",
      "- Patient Name: [Not provided]\n",
      "- Date of Birth: [Not provided]\n",
      "- Age: 58\n",
      "- Gender: Male\n",
      "- Weight: 74 kg\n",
      "- Height: 160 cm\n",
      "\n",
      "Medical History\n",
      "\n",
      "- Ischemic dilated cardiomyopathy\n",
      "- Dyslipemia\n",
      "- Myocardial Infarction\n",
      "- NYHA Class II\n",
      "\n",
      "Lab Results\n",
      "\n",
      "- Albumin: 404.0 g/L\n",
      "- ALT: 20 IU/L\n",
      "- AST: 20 IU/L\n",
      "- Total Cholesterol: 618 mmol/L\n",
      "- Creatinine: 121 mmol/L\n",
      "- Gamma-glutamil transpeptidase: 44.0 IU/L\n",
      "- Glucose: 56.0 mmol/L\n",
      "- Hemoglobin: 126.0 g/L\n",
      "- HDL: 0.98 mmol/L\n",
      "- Potassium: 4.6 mEq/L\n",
      "- LDL: 4.06 mmol/L\n",
      "- Sodium: 140.0 mEq/L\n",
      "- Pro-BNP: 570.0 ng/L\n",
      "- Protein: 75.0 g/L\n",
      "- T3: 0.04 pg/dL\n",
      "- T4: 12.0 ng/L\n",
      "- Troponin: 0.01 ng/mL\n",
      "- TSH: 3.27 mIU/L\n",
      "- Urea: 104.7 mg/dL\n",
      "\n",
      "Medications\n",
      "\n",
      "- Angiotensin II Receptor Blocker\n",
      "- Beta Blockers\n",
      "- Statins\n",
      "\n",
      "ECG Impressions\n",
      "\n",
      "- Ventricular Extrasystole: Monomorphic\n",
      "- Ventricular Tachycardia: No\n",
      "- Non-sustained ventricular tachycardia (CH>10): No\n",
      "- Paroxysmal supraventricular tachyarrhythmia: No\n",
      "- Bradycardia: No\n",
      "\n",
      "LVEF\n",
      "\n",
      "- Left Ventricular Ejection Fraction (LVEF): 35%\n",
      "\n",
      "Assessment and Plan\n",
      "\n",
      "Based on the patient's history of ischemic dilated cardiomyopathy, low LVEF, and elevated Pro-BNP levels, the patient is at high risk for heart failure. The presence of ventricular extrasystoles on the ECG also suggests underlying cardiac disease. However, the patient's current medications (Angiotensin II Receptor Blocker, Beta Blockers, Statins) are appropriate for managing heart failure and dyslipemia.\n",
      "\n",
      "Prediction\n",
      "\n",
      "- Confidence in survival for the next few years: 60%\n",
      "- Confidence in sudden cardiac death: 20%\n",
      "- Confidence in pump failure death: 20%\n",
      "\n",
      "The patient's low LVEF and history of myocardial infarction suggest a high risk for cardiac complications. However, the patient's current medications and the absence of ventricular tachycardia or non-sustained ventricular tachycardia on the ECG suggest that the patient is at lower risk for sudden cardiac death. The patient's Pro-BNP levels are elevated, indicating ongoing heart failure, but the patient's hemoglobin and albumin levels are within normal limits, suggesting that the patient is not in acute heart failure. Therefore, the patient's risk for pump failure death is also lower than sudden cardiac death.\n",
      "\n",
      "Reasoning for Prediction\n",
      "\n",
      "- Low LVEF and history of myocardial infarction increase the risk for cardiac complications\n",
      "- Current medications are appropriate for managing heart failure and dyslipemia\n",
      "- Elevated Pro-BNP levels suggest ongoing heart failure\n",
      "- Absence of ventricular tachycardia or non-sustained ventricular tachycardia on the ECG reduces the risk for sudden cardiac death\n",
      "- Hemoglobin and albumin levels are within normal limits, indicating that the patient is not in acute heart failure.\n"
     ]
    }
   ],
   "source": [
    "print(df_test['Reports'][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
